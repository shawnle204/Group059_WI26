{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# COGS 108 - Data Checkpoint"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Authors\n",
        "\n",
        "Instructions: REPLACE the contents of this cell with your team list and their contributions. Note that this will change over the course of the checkpoints\n",
        "\n",
        "This is a modified [CRediT taxonomy of contributions](https://credit.niso.org). For each group member please list how they contributed to this project using these terms:\n",
        "> Analysis, Background research, Conceptualization, Data curation, Experimental investigation, Methodology, Project administration, Software, Visualization, Writing – original draft, Writing – review & editing\n",
        "\n",
        "Example team list and credits:\n",
        "- Alice Anderson: Conceptualization, Data curation, Methodology, Writing - original draft\n",
        "- Bob Barker:  Analysis, Software, Visualization\n",
        "- Charlie Chang: Project administration, Software, Writing - review & editing\n",
        "- Dani Delgado: Analysis, Background research, Visualization, Writing - original draft"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Research Question"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To what extent do pre-college academic indicators predict first-year college academic performance? Using a dataset containing high school GPA, standardized test scores (SAT math and verbal), coursework rigor (e.g., number of AP/IB courses), and high school context (e.g., school competitiveness), we model students’ first-year college performance. This project evaluates the relative and combined predictive power of these variables using regression and classification approaches. We further analyze whether certain preparation profiles systematically overperform or underperform relative to model expectations. This predictive modeling task aims to identify which aspects of high school success transfer most effectively to early college performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Background and Prior Work"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Recent national discussions around college readiness have been brought into sharp focus at UC San Diego, where reports of a growing remedial math placement rate among incoming students have gained mainstream attention. As UCSD students, we were motivated by this issue after the Academic Senate released a Senate–Administration Working Group (SAWG) report analyzing admissions practices and student academic preparedness. The report highlights a notable increase in the number of incoming students requiring remedial-level math instruction, raising concerns about the alignment between pre-college preparation and college-level expectations.<a name=\"cite_ref-1\"></a><sup>1</sup>\n",
        " This development has prompted broader questions about how well traditional indicators of academic readiness translate into early college success.\n",
        "\n",
        "The SAWG report attributes part of this trend to changes in UCSD’s admissions landscape, including increased enrollment of students from under-resourced and lower-funded high schools.<a name=\"cite_ref-1\"></a><sup>1</sup>\n",
        " While the report does not claim that students from these schools are incapable of succeeding, it emphasizes that disparities in access to advanced coursework and academic resources may affect readiness for college-level mathematics. Importantly, the report suggests that commonly used admissions metrics may not fully capture differences in academic preparation, motivating further investigation into which pre-college factors meaningfully predict early college performance.\n",
        "\n",
        "Prior research and exploratory projects have examined similar questions using simplified academic datasets. One such example is a publicly documented Kaggle project that analyzed the relationship between high school GPA, SAT scores, and first-year college GPA using linear regression.<a name=\"cite_ref-2\"></a><sup>2</sup>\n",
        " That project found that high school GPA exhibited a slightly stronger correlation with first-year college GPA than SAT scores, suggesting that sustained academic performance may be a better indicator of early college success than standardized testing alone. The analysis also explored gender differences, observing small performance disparities despite similar SAT and high school GPA distributions.\n",
        "\n",
        "These findings align with broader education research suggesting that no single pre-college metric fully explains college outcomes. Geiser and Santelices (2007), using University of California admissions and outcomes data, report that high school GPA is a stronger and more consistent predictor of college success than standardized test scores across multiple outcomes.<a name=\"cite_ref-3\"></a><sup>3</sup>\n",
        " In a community college placement context, Belfield and Crosta (2012) compared placement-test-only approaches with transcript-based predictors and found that using high school performance indicators can improve placement decisions and reduce misclassification.<a name=\"cite_ref-4\"></a><sup>4</sup>\n",
        " More recent work by Allensworth and Clark (2020), which uses hierarchical modeling across high schools, shows that GPA remains a relatively stable predictor while test-score predictive power varies more across school contexts.<a name=\"cite_ref-5\"></a><sup>5</sup>\n",
        " Together, these methods-based findings suggest that model performance depends on both variable choice and institutional context, not just the algorithm itself.\n",
        "\n",
        "Building on both institutional reporting and prior data-driven projects, our work seeks to extend this line of inquiry by examining a broader set of pre-college academic indicators and their relationship to early college performance. In addition to SAT and high school GPA, prior studies motivate including variables tied to coursework rigor and educational context (for example, AP/IB exposure, first-generation status, and school-resource context) because these factors may mediate how test scores translate into first-year outcomes.<a name=\"cite_ref-4\"></a><sup>4</sup><a name=\"cite_ref-5\"></a><sup>,5</sup>\n",
        " By situating our analysis within the context of the UCSD remedial math discussion and grounding our variable selection in existing methods-focused literature, this project aims to better understand the practical limits of commonly used readiness metrics and identify which aspects of high school preparation are most strongly associated with first-year college success.\n",
        "\n",
        "References\n",
        "\n",
        "<a name=\"cite_note-1\"></a> 1\n",
        " UC San Diego Academic Senate. Report of the Senate–Administration Working Group on Admissions Review. UCSD, 2023. https://senate.ucsd.edu/media/740347/sawg-report-on-admissions-review-docs.pdf\n",
        "\n",
        "<a name=\"cite_note-2\"></a> 2\n",
        " Kaggle Discussion. Predicting College Success Using SAT and GPA. Kaggle, 2024. https://www.kaggle.com/discussions/getting-started/542760#3036652\n",
        "\n",
        "<a name=\"cite_note-3\"></a> 3\n",
        " Geiser, S., & Santelices, M. V. (2007). Validity of High-School Grades in Predicting Student Success Beyond the Freshman Year. UC Berkeley Center for Studies in Higher Education. https://cshe.berkeley.edu/publications/validity-high-school-grades-predicting-student-success-beyond-freshman-year-high-school\n",
        "\n",
        "<a name=\"cite_note-4\"></a> 4\n",
        " Belfield, C. R., & Crosta, P. M. (2012). Predicting Success in College: The Importance of Placement Tests and High School Transcripts. Community College Research Center, Teachers College, Columbia University. https://ccrc.tc.columbia.edu/publications/predicting-success-placement-tests-transcripts.html\n",
        "\n",
        "<a name=\"cite_note-5\"></a> 5\n",
        " Allensworth, E., & Clark, K. (2020). High school GPAs and ACT scores as predictors of college completion: Examining assumptions about consistency across high schools. Educational Researcher. https://doi.org/10.3102/0013189X20902110"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hypothesis\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our group hypothesises that high school GPA will be a stronger indicator for first-year college midterm scores than SAT scores after controlling for coursework. Students with high SAT scores but lower high school GPAs are expected not perform as well compared to our model predictions. On the other hand, students with high GPA and standard SAT scores are expected to perform higher, which suggests that sustained academic performance transfers much better to early college success than SAT alone."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1) \n",
        "- The ideal dataset for this project would include a range of academic and demographic variables carefully selected based on their established relationship with first-year college GPA in existing academic literature. The primary predictor variables would include SAT Score (Math and Verbal), High School GPA, number of AP/IB courses completed, demographic background, gender, and first-generation college student status; commonly cited factors towards college performance. A sample size of at least 5,000 entries is targeted to ensure the model has enough data to identify meaningful patterns, reduce variance in predictions, and produce results that generalize well beyond the training set. A smaller sample risks producing an underfitted or unreliable model, particularly when accounting for the diversity of demographic subgroups within the data.\n",
        "\n",
        "2) \n",
        "- Two datasets have currently been identified on key2stats.com, a platform that provides freely accessible statistical datasets for educational use. Both datasets were originally collected from university admissions and academic records, and each contains variables relevant to this project including first-year college GPA, cumulative high school GPA, SAT Math and SAT Verbal scores, first-generation status, demographic background, and gender. Details such as the exact number of samples per dataset and the specific institutions or years the data was drawn from are still being reviewed and will be confirmed upon closer inspection of the files.\n",
        "\n",
        "- A key consideration going forward is how these two datasets will be used in the modeling process. If the datasets share a consistent set of variables and were collected under similar conditions, they will be merged into a single unified dataset to maximize sample size and model robustness. If there are meaningful differences in how variables were recorded or defined across the two datasets, they will be treated and modeled separately, with results compared across both. This decision will be made after a thorough review of each dataset's structure, variable definitions, and any accompanying documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
        "#\n",
        "## this code is necessary for making sure that any modules we load are updated here \n",
        "## when their source code .py files are modified\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup code -- this only needs to be run once after cloning the repo!\n",
        "# this code downloads the data from its source to the `data/00-raw/` directory\n",
        "# if the data hasn't updated you don't need to do this again!\n",
        "\n",
        "# if you don't already have these packages (you should!) uncomment this line\n",
        "# %pip install requests tqdm\n",
        "\n",
        "import sys\n",
        "sys.path.append('./modules') # this tells python where to look for modules to import\n",
        "\n",
        "import get_data # this is where we get the function we need to download data\n",
        "\n",
        "# replace the urls and filenames in this list with your actual datafiles\n",
        "# yes you can use Google drive share links or whatever\n",
        "# format is a list of dictionaries; \n",
        "# each dict has keys of \n",
        "#   'url' where the resource is located\n",
        "#   'filename' for the local filename where it will be stored \n",
        "datafiles = [\n",
        "    { 'url': 'https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/airline-safety/airline-safety.csv', 'filename':'airline-safety.csv'},\n",
        "    { 'url': 'https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/bad-drivers/bad-drivers.csv', 'filename':'bad-drivers.csv'}\n",
        "]\n",
        "\n",
        "get_data.get_raw(datafiles,destination_directory='data/00-raw/')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Highschool to College 2009 Longitudinal Study Data\n",
        "\n",
        "Instructions: \n",
        "1. Change the header from Dataset #1 to something more descriptive of the dataset\n",
        "2. Write a few paragraphs about this dataset. Make sure to cover\n",
        "   1. Describe the important metrics, what units they are in, and giv some sense of what they mean.  For example \"Fasting blood glucose in units of mg glucose per deciliter of blood.  Normal values for healthy individuals range from 70 to 100 mg/dL.  Values 100-125 are prediabetic and values >125mg/dL indicate diabetes. Values <70 indicate hypoglycemia. Fasting idicates the patient hasn't eaten in the last 8 hours.  If blood glucose is >250 or <50 at any time (regardless of the time of last meal) the patient's life may be in immediate danger\"\n",
        "   2. If there are any major concerns with the dataset, describe them. For example \"Dataset is composed of people who are serious enough about eating healthy that they voluntarily downloaded an app dedicated to tracking their eating patterns. This sample is likely biased because of that self-selection. These people own smartphones and may be healthier and may have more disposable income than the average person.  Those who voluntarily log conscientiously and for long amounts of time are also likely even more interested in health than those who download the app and only log a bit before getting tired of it\"\n",
        "3. Use the cell below to \n",
        "    1. load the dataset \n",
        "    2. make the dataset tidy or demonstrate that it was already tidy\n",
        "    3. demonstrate the size of the dataset\n",
        "    4. find out how much data is missing, where its missing, and if its missing at random or seems to have any systematic relationships in its missingness\n",
        "    5. find and flag any outliers or suspicious entries\n",
        "    6. clean the data or demonstrate that it was already clean.  You may choose how to deal with missingness (dropna of fillna... how='any' or 'all') and you should justify your choice in some way\n",
        "    7. You will load raw data from `data/00-raw/`, you will (optionally) write intermediate stages of your work to `data/01-interim` and you will write the final fully wrangled version of your data to `data/02-processed`\n",
        "4. Optionally you can also show some summary statistics for variables that you think are important to the project\n",
        "5. Feel free to add more cells here if that's helpful for you\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Csv file is too large to be pushed into git, so download it here\n",
        "https://drive.google.com/file/d/1avtIqehIbvZ_4D7bibednlwA4rfM24GT/view?usp=sharing  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw shape: (23503, 9614)\n",
            "Dropped all -5 columns: 932\n",
            "Dropped loan columns: 21\n",
            "Dropped constant columns: 1\n",
            "Dropped high-missing columns (>=80% missing): 0\n",
            "Cleaned shape: (23503, 8660)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>STU_ID</th>\n",
              "      <th>X2UNIV1</th>\n",
              "      <th>X2UNIV2A</th>\n",
              "      <th>X2UNIV2B</th>\n",
              "      <th>X3UNIV1</th>\n",
              "      <th>X4UNIV1</th>\n",
              "      <th>W1STUDENT</th>\n",
              "      <th>W1PARENT</th>\n",
              "      <th>W1MATHTCH</th>\n",
              "      <th>W1SCITCH</th>\n",
              "      <th>...</th>\n",
              "      <th>W5W1W2W3W4PSRECORDS191</th>\n",
              "      <th>W5W1W2W3W4PSRECORDS192</th>\n",
              "      <th>W5W1W2W3W4PSRECORDS193</th>\n",
              "      <th>W5W1W2W3W4PSRECORDS194</th>\n",
              "      <th>W5W1W2W3W4PSRECORDS195</th>\n",
              "      <th>W5W1W2W3W4PSRECORDS196</th>\n",
              "      <th>W5W1W2W3W4PSRECORDS197</th>\n",
              "      <th>W5W1W2W3W4PSRECORDS198</th>\n",
              "      <th>W5W1W2W3W4PSRECORDS199</th>\n",
              "      <th>W5W1W2W3W4PSRECORDS200</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10001</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1111</td>\n",
              "      <td>11111</td>\n",
              "      <td>375.667105</td>\n",
              "      <td>470.250141</td>\n",
              "      <td>423.238620</td>\n",
              "      <td>393.169508</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2098.087446</td>\n",
              "      <td>1824.641398</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2431.665487</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2457.423209</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2053.40787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10002</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1111</td>\n",
              "      <td>11111</td>\n",
              "      <td>189.309446</td>\n",
              "      <td>224.455466</td>\n",
              "      <td>329.640843</td>\n",
              "      <td>207.892322</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10003</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1111</td>\n",
              "      <td>11111</td>\n",
              "      <td>143.591863</td>\n",
              "      <td>185.301339</td>\n",
              "      <td>231.718703</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10004</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1001</td>\n",
              "      <td>10011</td>\n",
              "      <td>227.937019</td>\n",
              "      <td>301.431713</td>\n",
              "      <td>261.518593</td>\n",
              "      <td>306.102816</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10005</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1111</td>\n",
              "      <td>11111</td>\n",
              "      <td>145.019401</td>\n",
              "      <td>190.834136</td>\n",
              "      <td>169.946035</td>\n",
              "      <td>188.432535</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 8660 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   STU_ID  X2UNIV1  X2UNIV2A  X2UNIV2B  X3UNIV1  X4UNIV1   W1STUDENT  \\\n",
              "0   10001       11         1         1     1111    11111  375.667105   \n",
              "1   10002       11         1         1     1111    11111  189.309446   \n",
              "2   10003       11         1         1     1111    11111  143.591863   \n",
              "3   10004       10         1         7     1001    10011  227.937019   \n",
              "4   10005       11         1         1     1111    11111  145.019401   \n",
              "\n",
              "     W1PARENT   W1MATHTCH    W1SCITCH  ...  W5W1W2W3W4PSRECORDS191  \\\n",
              "0  470.250141  423.238620  393.169508  ...                     0.0   \n",
              "1  224.455466  329.640843  207.892322  ...                     0.0   \n",
              "2  185.301339  231.718703    0.000000  ...                     0.0   \n",
              "3  301.431713  261.518593  306.102816  ...                     0.0   \n",
              "4  190.834136  169.946035  188.432535  ...                     0.0   \n",
              "\n",
              "   W5W1W2W3W4PSRECORDS192  W5W1W2W3W4PSRECORDS193  W5W1W2W3W4PSRECORDS194  \\\n",
              "0             2098.087446             1824.641398                     0.0   \n",
              "1                0.000000                0.000000                     0.0   \n",
              "2                0.000000                0.000000                     0.0   \n",
              "3                0.000000                0.000000                     0.0   \n",
              "4                0.000000                0.000000                     0.0   \n",
              "\n",
              "   W5W1W2W3W4PSRECORDS195  W5W1W2W3W4PSRECORDS196  W5W1W2W3W4PSRECORDS197  \\\n",
              "0             2431.665487                     0.0                     0.0   \n",
              "1                0.000000                     0.0                     0.0   \n",
              "2                0.000000                     0.0                     0.0   \n",
              "3                0.000000                     0.0                     0.0   \n",
              "4                0.000000                     0.0                     0.0   \n",
              "\n",
              "   W5W1W2W3W4PSRECORDS198  W5W1W2W3W4PSRECORDS199  W5W1W2W3W4PSRECORDS200  \n",
              "0             2457.423209                     0.0              2053.40787  \n",
              "1                0.000000                     0.0                 0.00000  \n",
              "2                0.000000                     0.0                 0.00000  \n",
              "3                0.000000                     0.0                 0.00000  \n",
              "4                0.000000                     0.0                 0.00000  \n",
              "\n",
              "[5 rows x 8660 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "X5PFYSEC          0.001957\n",
              "X5REPLN           0.000894\n",
              "W3STUDENTTR117    0.000000\n",
              "W3STUDENTTR111    0.000000\n",
              "W3STUDENTTR112    0.000000\n",
              "W3STUDENTTR113    0.000000\n",
              "W3STUDENTTR114    0.000000\n",
              "W3STUDENTTR115    0.000000\n",
              "W3STUDENTTR116    0.000000\n",
              "STU_ID            0.000000\n",
              "W3STUDENTTR125    0.000000\n",
              "W3STUDENTTR119    0.000000\n",
              "W3STUDENTTR120    0.000000\n",
              "W3STUDENTTR121    0.000000\n",
              "W3STUDENTTR122    0.000000\n",
              "W3STUDENTTR123    0.000000\n",
              "W3STUDENTTR118    0.000000\n",
              "W3STUDENTTR110    0.000000\n",
              "W3STUDENTTR109    0.000000\n",
              "W3STUDENTTR108    0.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "csv_path = Path(\"hsls_17_student_pets_sr_v1_0.csv\")\n",
        "assert csv_path.exists(), f\"Missing file: {csv_path.resolve()}\"\n",
        "\n",
        "# 1) Load raw data\n",
        "df_raw = pd.read_csv(csv_path, low_memory=False)\n",
        "print(\"Raw shape:\", df_raw.shape)\n",
        "\n",
        "# 2) Drop variables documented as Restricted-Use-File (RUF) only\n",
        "restricted_use_vars = [\n",
        "    \"STU_ID\", \"I5IPEDSID\", \"I5SRSUBMIT\", \"I5SRSUBMITMODE\", \"X5SRRESP\",\n",
        "    \"R5SRSTAT\", \"R5SPFRECORD\", \"X5PFYEAR\", \"R5SDOBYM\", \"R5SSEX\",\n",
        "    \"R5SMARITAL\", \"R5SCITIZEN\", \"R5SVETERAN\", \"R5SHIGHSCH\", \"R5SHIGHYR\",\n",
        "    \"R5SHISPAN\", \"R5SWHITE\", \"R5SBLACK\", \"R5SASIAN\", \"R5SINDIAN\",\n",
        "    \"R5SISLAND\", \"R5SPERMRES\", \"R5SCHSTRES\", \"R5SFRSTYM\", \"R5SLSTYM\",\n",
        "    \"R5SFTB\", \"R5STRANSFER\", \"R5SREMEVR\", \"R5SENRL12\", \"R5SENRL13\",\n",
        "    \"R5SENRL14\", \"R5SENRL15\", \"R5SENRL16\", \"R5SENRL17\", \"R5SENROLLSTR\",\n",
        "    \"R5SBAREC\", \"R5SBARECDTE\", \"R5SDEGEXP\"\n",
        "]\n",
        "present_restricted = [c for c in restricted_use_vars if c in df_raw.columns]\n",
        "df = df_raw.drop(columns=present_restricted).copy()\n",
        "\n",
        "# 3) Drop columns that are entirely sentinel -5\n",
        "all_neg5_cols = []\n",
        "for col in df.columns:\n",
        "    col_as_str = df[col].astype(str).str.strip()\n",
        "    non_na = col_as_str[col_as_str.ne(\"nan\")]\n",
        "    if len(non_na) > 0 and non_na.eq(\"-5\").all():\n",
        "        all_neg5_cols.append(col)\n",
        "df = df.drop(columns=all_neg5_cols)\n",
        "\n",
        "# 4) Drop loan-related columns\n",
        "loan_cols = [c for c in df.columns if \"loan\" in c.lower()]\n",
        "df = df.drop(columns=loan_cols)\n",
        "\n",
        "# 5) Treat remaining -5 as missing\n",
        "df = df.replace(-5, pd.NA).replace(\"-5\", pd.NA)\n",
        "\n",
        "# 6) Drop low-information columns\n",
        "constant_cols = [c for c in df.columns if df[c].nunique(dropna=True) <= 1]\n",
        "df = df.drop(columns=constant_cols)\n",
        "\n",
        "# 7) Drop high-missing columns\n",
        "missing_threshold = 0.80\n",
        "missing_ratio = df.isna().mean()\n",
        "high_missing_cols = missing_ratio[missing_ratio >= missing_threshold].index.tolist()\n",
        "df = df.drop(columns=high_missing_cols)\n",
        "\n",
        "# 8) Keep a focused set of 50 non-restricted, college-success-relevant variables\n",
        "selected_vars = [\n",
        "    \"X1SEX\", \"X1RACE\", \"X1HISPANIC\", \"X1WHITE\", \"X1BLACK\",\n",
        "    \"X1SES\", \"X1FAMINCOME\", \"X1PAREDU\", \"X1PAR1EDU\", \"X1PAR2EDU\",\n",
        "    \"X1POVERTY130\", \"X1POVERTY185\", \"X1LOCALE\", \"X1CONTROL\",\n",
        "    \"X1SCHOOLBEL\", \"X1SCHOOLENG\", \"X1STUEDEXPCT\", \"X1PAREDEXPCT\",\n",
        "    \"X1MTHID\", \"X1MTHEFF\", \"X1SCIID\", \"X1SCIEFF\",\n",
        "    \"X1TXMTSCOR\", \"X2TXMTSCOR\", \"X2EVERDROP\",\n",
        "    \"X3TGPAWGT\", \"X3TGPATOT\", \"X3TCREDTOT\", \"X3TCREDSTEM\", \"X3TCREDAPIB\",\n",
        "    \"X3TCREDAPMTH\", \"X3TCREDAPSCI\", \"X3TCREDMAT\", \"X3TCREDSCI\", \"X3TCREDENG\",\n",
        "    \"S1SAT\", \"S1ACT\", \"S2SATNUM\", \"S2IMPCLGEXAM\",\n",
        "    \"S1HRMHOMEWK\", \"S1HRSHOMEWK\", \"S1GOODGRADES\", \"S2ABSENT\", \"S2LATESCH\", \"S2INSCHSUSP\",\n",
        "    \"X5YR1GPA\", \"X5YR1ATT\", \"X5YR1ERN\", \"X5REMRAT\", \"X5ENRATT\"\n",
        "]\n",
        "\n",
        "missing_selected = [c for c in selected_vars if c not in df.columns]\n",
        "df_model = df[[c for c in selected_vars if c in df.columns]].copy()\n",
        "\n",
        "# Summary\n",
        "print(\"Dropped restricted-use columns present in this file:\", len(present_restricted))\n",
        "print(\"Dropped all -5 columns:\", len(all_neg5_cols))\n",
        "print(\"Dropped loan columns:\", len(loan_cols))\n",
        "print(\"Dropped constant columns:\", len(constant_cols))\n",
        "print(f\"Dropped high-missing columns (>={missing_threshold:.0%} missing):\", len(high_missing_cols))\n",
        "print(\"Wide cleaned shape:\", df.shape)\n",
        "print(\"Selected vars requested:\", len(selected_vars))\n",
        "print(\"Selected vars found:\", df_model.shape[1])\n",
        "print(\"Model dataframe shape:\", df_model.shape)\n",
        "if missing_selected:\n",
        "    print(\"Missing selected vars:\", missing_selected)\n",
        "\n",
        "display(df_model.head())\n",
        "display(df_model.isna().mean().sort_values(ascending=False).head(20))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset #2 \n",
        "\n",
        "See instructions above for Dataset #1.  Feel free to keep adding as many more datasets as you need.  Put each new dataset in its own section just like these. \n",
        "\n",
        "Lastly if you do have multiple datasets, add another section where you demonstrate how you will join, align, cross-reference or whatever to combine data from the different datasets\n",
        "\n",
        "Please note that you can always keep adding more datasets in the future if these datasets you turn in for the checkpoint aren't sufficient.  The goal here is demonstrate that you can obtain and wrangle data.  You are not tied down to only use what you turn in right now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ethics"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ethics \n",
        "\n",
        "Instructions: Keep the contents of this cell. For each item on the checklist\n",
        "-  put an X there if you've considered the item\n",
        "-  IF THE ITEM IS RELEVANT place a short paragraph after the checklist item discussing the issue.\n",
        "  \n",
        "Items on this checklist are meant to provoke discussion among good-faith actors who take their ethical responsibilities seriously. Your teams will document these discussions and decisions for posterity using this section.  You don't have to solve these problems, you just have to acknowledge any potential harm no matter how unlikely.\n",
        "\n",
        "Here is a [list of real world examples](https://deon.drivendata.org/examples/) for each item in the checklist that can refer to.\n",
        "\n",
        "[![Deon badge](https://img.shields.io/badge/ethics%20checklist-deon-brightgreen.svg?style=popout-square)](http://deon.drivendata.org/)\n",
        "\n",
        "### A. Data Collection\n",
        " - [X] **A.1 Informed consent**: If there are human subjects, have they given informed consent, where subjects affirmatively opt-in and have a clear understanding of the data uses to which they consent?\n",
        "> All students are aware that their data would be used. No data was collected through force or secrecy.\n",
        " - [ ] **A.2 Collection bias**: Have we considered sources of bias that could be introduced during data collection and survey design and taken steps to mitigate those?\n",
        " \n",
        " - [X] **A.3 Limit PII exposure**: Have we considered ways to minimize exposure of personally identifiable information (PII) for example through anonymization or not collecting information that isn't relevant for analysis?\n",
        "> No names, contact information, or other identifiers were collected or stored. Students will be differentiated using a unique number identifier, eliminating the possibility of PII.\n",
        " - [ ] **A.4 Downstream bias mitigation**: Have we considered ways to enable testing downstream results for biased outcomes (e.g., collecting data on protected group status like race or gender)?\n",
        "\n",
        "### B. Data Storage\n",
        " - [X] **B.1 Data security**: Do we have a plan to protect and secure data (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?\n",
        "> All data is from public sources and contains no PII; access is limited to project members.\n",
        " - [X] **B.2 Right to be forgotten**: Do we have a mechanism through which an individual can request their personal information be removed?\n",
        " > No, individuals will not be able to request for the removal of their data.\n",
        " - [X] **B.3 Data retention plan**: Is there a schedule or plan to delete the data after it is no longer needed?\n",
        " > Temporary copies of the dataset will be deleted from local machines once grading is complete. This minimizes long-term risk of unintended reuse. No archival use of the data is planned. Retention is strictly tied to the academic lifecycle of the project.\n",
        "\n",
        "### C. Analysis\n",
        " - [ ] **C.1 Missing perspectives**: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)?\n",
        " > We have no relevant stakeholders. \n",
        " - [X] **C.2 Dataset bias**: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)?\n",
        " > These biases could influence correlations observed in the analysis. So, findings will be framed as descriptive rather than prescriptive. The analysis avoids claims about inherent ability or merit. Observed patterns are interpreted as outcomes shaped by broader educational contexts.\n",
        " - [X] **C.3 Honest representation**: Are our visualizations, summary statistics, and reports designed to honestly represent the underlying data?\n",
        "> Axes, scales, and labels are chosen carefully to avoid misleading interpretations. Any uncertainty or variability in the data will be clearly shown when relevant. Visualizations will not be optimized to support a predetermined narrative. Alternative explanations for trends will be acknowledged.\n",
        " - [X] **C.4 Privacy in analysis**: Have we ensured that data with PII are not used or displayed unless necessary for the analysis?\n",
        " > No personally identifiable information will be used or displayed during analysis.\n",
        " - [X] **C.5 Auditability**: Is the process of generating the analysis well documented and reproducible if we discover issues in the future?\n",
        " > Code and analysis steps are version-controlled and clearly commented. This allows future reviewers to trace decisions and identify potential issues. Assumptions made during preprocessing aredocumented.\n",
        "\n",
        "### D. Modeling\n",
        " - [X] **D.1 Proxy discrimination**: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory?\n",
        " > Features correlated with socioeconomic status or institutional advantage are not considered in the analysis, making it impossible for the model to rely on discriminatory variables.\n",
        " - [ ] **D.2 Fairness across groups**: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)?\n",
        " > We have not yet tested any model results.\n",
        " - [X] **D.3 Metric selection**: Have we considered the effects of optimizing for our defined metrics and considered additional metrics?\n",
        " > Tradeoffs between accuracy, interpretability, and robustness are considered. Metrics are chosen to reflect the exploratory goals of the project. No single metric is treated as definitive. Performance results are interpreted within context rather than as absolute measures.\n",
        " - [X] **D.4 Explainability**: Can we explain in understandable terms a decision the model made in cases where a justification is needed?\n",
        " > Complex models are avoided where simpler explanations suffice. When predictions are discussed, their uncertainty is emphasized as to reduce the risk of inappropriate reliance on the model.\n",
        " - [] **D.5 Communicate limitations**: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood?\n",
        "\n",
        "### E. Deployment\n",
        " - [ ] **E.1 Monitoring and evaluation**: Do we have a clear plan to monitor the model and its impacts after it is deployed (e.g., performance monitoring, regular audit of sample predictions, human review of high-stakes decisions, reviewing downstream impacts of errors or low-confidence decisions, testing for concept drift)?\n",
        " > We will not be deploying the model. For this reason, this ethical consideration is irrelevant.\n",
        " - [ ] **E.2 Redress**: Have we discussed with our organization a plan for response if users are harmed by the results (e.g., how does the data science team evaluate these cases and update analysis and models to prevent future harm)?\n",
        " > Since the model will not be deployed, this ethical issue does not apply.\n",
        " - [ ] **E.3 Roll back**: Is there a way to turn off or roll back the model in production if necessary?\n",
        " > As the model will not be used in practice, this ethical consideration can be disregarded.\n",
        " - [ ] **E.4 Unintended use**: Have we taken steps to identify and prevent unintended uses and abuse of the model and do we have a plan to monitor these once the model is deployed?\n",
        " > The model will not be implemented, so this ethical point is not pertinent.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Team Expectations "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* *Everyone will contribute to their greatest extent on this project.*\n",
        "* *Complete assigned task in a reasonable time frame (24-48 hours before next checkpoint).*\n",
        "* *Communicate through iMessage for any concerns.*\n",
        "* *Communicate newly added commits beforehand and gain approval from team members.*\n",
        "* *Communicate obstructions or emergencies.*\n",
        "* *Team members will address or assist anyone in any concern (or seek assistance from TA/Professor for complications)*"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Project Timeline Proposal"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Specify your team's specific project timeline. Calendar starts 2/03 (initial project proposal) and ends 3/20 (final submission). Update times and task owners as needed.\n",
        "\n",
        "| Meeting Date  | Meeting Time | Completed Before Meeting | Discuss at Meeting |\n",
        "|---|---:|---|---|\n",
        "| 2/03 | 5:00 PM | Initial project proposal draft; assign roles (All: Jaden, Trenton, Juhoan, Minh, Shawn) | Finalize research question, hypothesis, and timeline |\n",
        "| 2/10 | 6:00 PM | Background research & locate candidate datasets (Trenton, Juhoan) | Review dataset options, data access steps, and ethics considerations |\n",
        "| 2/17 | 6:00 PM | Download & store raw data into `data/00-raw/` (Jaden) | Review data schema and outline wrangling tasks; assign wrangling leads |\n",
        "| 2/24 | 6:00 PM | Wrangling & initial EDA completed for a sample subset (Minh) | Review cleaned data, refine EDA, choose features for analysis |\n",
        "| 3/03 | 6:00 PM | Complete wrangling & full EDA report draft (Shawn, Jaden) | Finalize analysis plan and modeling approach; split analysis tasks |\n",
        "| 3/10 | 6:00 PM | Run initial analyses/models; produce preliminary results (Trenton, Minh) | Review model performance, iterate, and prepare figures for results |\n",
        "| 3/17 | 6:00 PM | Draft results, figures, and discussion; begin final write-up (All) | Peer-review full project, address feedback, and finalize edits |\n",
        "| 3/20 | Before 11:59 PM | Final project PDF, notebook, and code pushed to repo; group surveys submitted | Turn in Final Project & Group Project Surveys |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
